# Celery + Redis èƒŒæ™¯è¨“ç·´ç³»çµ± - å¯¦ä½œè¨ˆç•«

## ç›®æ¨™
å°‡åŒæ­¥çš„è¨“ç·´ç«¯é» (`POST /train`) æ”¹ç‚ºéåŒæ­¥åŸ·è¡Œï¼Œä½¿ç”¨ Celery + Redis å¯¦ç¾èƒŒæ™¯è¨“ç·´ï¼Œç™¼å¸ƒç‚º **v2.0.0**ï¼ˆBreaking Changeï¼‰ã€‚

---

## ğŸ“‹ é—œéµæ±ºç­–ç¸½çµ

| é …ç›® | æ±ºç­– |
|------|------|
| Worker æ•¸é‡ | 2 å€‹ (å¯åŒæ™‚è¨“ç·´ 2 å€‹æ¨¡å‹) |
| ä¸¦ç™¼é™åˆ¶ | Worker `--concurrency=3` |
| Job ä¿ç•™æ™‚é–“ | succeeded: 90å¤©, failed: 30å¤© |
| Celery Flower | æš«ä¸å®‰è£ |
| Job Storage | JSON Filesï¼ˆèˆ‡ç¾æœ‰æ¶æ§‹ä¸€è‡´ï¼‰ |
| å‘å¾Œç›¸å®¹æ€§ | âŒ å®Œå…¨éåŒæ­¥ï¼ˆBreaking Changeï¼‰ |
| Retrain ç«¯é» | ä¸ä¿®æ”¹ï¼ˆä¿æŒåŸç‹€æˆ–è€ƒæ…®å»¢é™¤ï¼‰ |

---

## ğŸ”„ æ ¸å¿ƒæ”¹è®Š

### API è¡Œç‚ºè®Šæ›´
**Before (v1.x - åŒæ­¥):**
```python
POST /train â†’ ç­‰å¾…è¨“ç·´å®Œæˆ (30-60ç§’) â†’ å›å‚³ model_id
```

**After (v2.0 - éåŒæ­¥):**
```python
POST /train â†’ ç«‹å³å›å‚³ job_id
GET /jobs/{job_id} â†’ æŸ¥è©¢ç‹€æ…‹ â†’ succeeded æ™‚å–å¾— model_id
```

### æ–°å¢ API ç«¯é»
- `GET /jobs` - åˆ—å‡ºè¨“ç·´ä»»å‹™
- `GET /jobs/{job_id}` - æŸ¥è©¢ä»»å‹™ç‹€æ…‹
- `DELETE /jobs/{job_id}` - å–æ¶ˆ/åˆªé™¤ä»»å‹™

---

## ğŸ—‚ï¸ å¯¦ä½œè®Šæ›´è©³ç´°èªªæ˜

### Phase 1: åŸºç¤è¨­æ–½è¨­å®š

#### [NEW] [docker-compose.yml](file:///c:/Users/ander/Documents/GitHub/XGenerator/docker-compose.yml)
å‰µå»º Docker Compose é…ç½®ï¼ŒåŒ…å«ä¸‰å€‹æœå‹™ï¼š
- **api**: ç¾æœ‰çš„ FastAPI æ‡‰ç”¨ï¼ˆport 8000ï¼‰
- **redis**: Redis ä½œç‚º message brokerï¼ˆport 6379ï¼‰
- **worker**: Celery worker é€²ç¨‹ï¼ˆèƒŒæ™¯è¨“ç·´ï¼‰

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env
    depends_on:
      - redis
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  worker:
    build: .
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env
    depends_on:
      - redis
    command: celery -A app.celery_app worker --loglevel=info --concurrency=2
```

> **æ³¨æ„**: å…©å€‹ worker é€²ç¨‹ (`--concurrency=2`)ï¼Œå…±äº« `./data` ç›®éŒ„ä»¥å­˜å–è³‡æ–™é›†å’Œæ¨¡å‹ã€‚

---

#### [MODIFY] [requirements.txt](file:///c:/Users/ander/Documents/GitHub/XGenerator/requirements.txt)
æ–°å¢ Celery å’Œ Redis ä¾è³´ï¼š

```diff
 fastapi==0.109.0
 uvicorn[standard]==0.27.0
 pydantic==2.5.3
 python-multipart==0.0.6
 
 xgboost==2.0.3
 scikit-learn==1.4.0
 pandas==2.2.0
 numpy==1.26.3
 joblib==1.3.2
 
 openai==1.6.1
 python-dotenv==1.0.0
 slowapi==0.1.9
+
+# Celery + Redis for background tasks
+celery==5.3.4
+redis==5.0.1
```

---

#### [MODIFY] [app/config.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/config.py)
æ–°å¢ Redis URL å’Œ Jobs ç›®éŒ„é…ç½®ï¼š

```python
# æ–°å¢
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# æ–°å¢ Jobs ç›®éŒ„
JOBS_DIR = DATA_DIR / "jobs"
JOBS_DIR.mkdir(exist_ok=True)
```

---

#### [NEW] [app/celery_app.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/celery_app.py)
å‰µå»º Celery æ‡‰ç”¨é…ç½®ï¼š

```python
"""Celery application configuration"""
from celery import Celery
from app.config import REDIS_URL

# Create Celery app
celery_app = Celery(
    "xgenerator_tasks",
    broker=REDIS_URL,
    backend=REDIS_URL
)

# Configuration
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=3600,  # 1 hour max per task
    task_soft_time_limit=3300,  # 55 minutes soft limit
)

# Auto-discover tasks
celery_app.autodiscover_tasks(['app.tasks'])
```

> **é—œéµé…ç½®**:
> - `task_time_limit`: å–®å€‹è¨“ç·´ä»»å‹™æœ€å¤š 1 å°æ™‚
> - `task_track_started`: è¿½è¹¤ä»»å‹™é–‹å§‹æ™‚é–“
> - Auto-discover tasks from `app.tasks` module

---

### Phase 2: Job ç®¡ç†ç³»çµ±

#### [MODIFY] [app/models/schemas.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/models/schemas.py)
æ–°å¢ Job ç›¸é—œçš„ Pydantic æ¨¡å‹ï¼š

```python
from typing import Literal

# Job Status Enum
JobStatus = Literal["queued", "running", "succeeded", "failed", "cancelled"]

# Job Create Response (when POST /train)
class JobCreateResponse(BaseModel):
    """Response when creating a training job"""
    job_id: str
    status: JobStatus
    created_at: str
    message: str = "Training job created. Use GET /jobs/{job_id} to check status."

# Job Result (when succeeded)
class JobResult(BaseModel):
    """Job result when training succeeds"""
    model_id: str
    metrics: Optional[Dict[str, float]] = None
    training_duration: float

# Job Error (when failed)
class JobError(BaseModel):
    """Job error details when training fails"""
    error_type: str
    message: str
    details: Optional[Dict[str, Any]] = None

# Job Detail (GET /jobs/{job_id})
class JobDetail(BaseModel):
    """Detailed job information"""
    job_id: str
    job_type: Literal["train", "retrain"] = "train"
    status: JobStatus
    user_id: str
    created_at: str
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    
    # Training configuration
    train_config: Dict[str, Any]
    
    # Results (only when succeeded)
    result: Optional[JobResult] = None
    
    # Error info (only when failed)
    error: Optional[JobError] = None
    
    # Celery task ID (for debugging)
    celery_task_id: Optional[str] = None

# Job Summary (for listing)
class JobSummary(BaseModel):
    """Summary of a job for listing"""
    job_id: str
    job_type: Literal["train", "retrain"] = "train"
    status: JobStatus
    user_id: str
    model_name: str
    created_at: str
    completed_at: Optional[str] = None
    model_id: Optional[str] = None  # Only when succeeded

# Job List Response
class JobListResponse(BaseModel):
    """Response for listing jobs"""
    jobs: List[JobSummary]
    total: int
    limit: int
    offset: int
```

---

#### [NEW] [app/services/job_service.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/services/job_service.py)
å¯¦ä½œ Job CRUD å‡½æ•¸ï¼š

```python
"""Job management service"""
import json
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

from app.config import JOBS_DIR
from app.utils.file_utils import generate_id, atomic_write_json

def create_job(
    user_id: str,
    train_config: Dict[str, Any],
    celery_task_id: str
) -> Dict[str, Any]:
    """Create a new training job"""
    job_id = generate_id()
    job_data = {
        "job_id": job_id,
        "job_type": "train",
        "status": "queued",
        "user_id": user_id,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "started_at": None,
        "completed_at": None,
        "train_config": train_config,
        "result": None,
        "error": None,
        "celery_task_id": celery_task_id
    }
    
    job_path = JOBS_DIR / f"job_{job_id}.json"
    atomic_write_json(job_path, job_data)
    return job_data

def get_job(job_id: str) -> Optional[Dict[str, Any]]:
    """Get job by ID"""
    job_path = JOBS_DIR / f"job_{job_id}.json"
    if not job_path.exists():
        return None
    
    with open(job_path, "r") as f:
        return json.load(f)

def update_job_status(
    job_id: str,
    status: str,
    **kwargs
) -> None:
    """Update job status and other fields"""
    job_data = get_job(job_id)
    if not job_data:
        raise ValueError(f"Job {job_id} not found")
    
    job_data["status"] = status
    
    # Update timestamps
    if status == "running" and not job_data.get("started_at"):
        job_data["started_at"] = datetime.utcnow().isoformat() + "Z"
    
    if status in ["succeeded", "failed", "cancelled"]:
        job_data["completed_at"] = datetime.utcnow().isoformat() + "Z"
    
    # Update other fields
    for key, value in kwargs.items():
        job_data[key] = value
    
    job_path = JOBS_DIR / f"job_{job_id}.json"
    atomic_write_json(job_path, job_data)

def list_jobs(
    user_id: Optional[str] = None,
    status: Optional[str] = None,
    limit: int = 50,
    offset: int = 0
) -> tuple[List[Dict[str, Any]], int]:
    """List jobs with filters"""
    all_jobs = []
    
    for job_file in sorted(JOBS_DIR.glob("job_*.json"), reverse=True):
        try:
            with open(job_file, "r") as f:
                job_data = json.load(f)
            
            # Apply filters
            if user_id and job_data.get("user_id") != user_id:
                continue
            if status and job_data.get("status") != status:
                continue
            
            all_jobs.append(job_data)
        except Exception:
            continue
    
    total = len(all_jobs)
    paginated_jobs = all_jobs[offset:offset + limit]
    
    return paginated_jobs, total

def delete_job(job_id: str) -> bool:
    """Delete a job"""
    job_path = JOBS_DIR / f"job_{job_id}.json"
    if not job_path.exists():
        return False
    
    job_path.unlink()
    return True
```

---

### Phase 3: Celery Worker å¯¦ä½œ

#### [NEW] [app/tasks/training_tasks.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/tasks/training_tasks.py)
å¯¦ä½œè¨“ç·´ä»»å‹™ï¼ˆæ ¸å¿ƒé‚è¼¯ï¼‰ï¼š

```python
"""Celery tasks for model training"""
import pandas as pd
import joblib
import json
import time
from datetime import datetime
from pathlib import Path

from app.celery_app import celery_app
from app.services.job_service import update_job_status, get_job
from app.services.training_service import (
    validate_features_and_target,
    train_model_with_validation
)
from app.config import (
    DATASETS_DIR, DATASET_METADATA_DIR, ARTIFACTS_DIR,
    MODEL_METADATA_DIR, MODEL_SCHEMAS_DIR
)
from app.utils.file_utils import generate_id, atomic_write_json

@celery_app.task(bind=True, max_retries=3)
def train_model_task(self, job_id: str):
    """
    Background task to train a model.
    
    Args:
        job_id: The job ID to track progress
    
    Returns:
        model_id: The ID of the trained model
    """
    try:
        # Update status to running
        update_job_status(job_id, "running")
        
        # Load job config
        job_data = get_job(job_id)
        if not job_data:
            raise ValueError(f"Job {job_id} not found")
        
        config = job_data["train_config"]
        user_id = job_data["user_id"]
        
        start_time = time.time()
        
        # 1. Load dataset
        dataset_path = DATASETS_DIR / f"{config['dataset_id']}.csv"
        if not dataset_path.exists():
            raise FileNotFoundError(f"Dataset {config['dataset_id']} not found")
        
        df = pd.read_csv(dataset_path)
        
        # 2. Load dataset schema
        schema_path = DATASET_METADATA_DIR / f"schema_{config['dataset_id']}.json"
        with open(schema_path, "r") as f:
            schema = json.load(f)
        
        # 3. Validate features and target
        features = validate_features_and_target(
            df=df,
            schema=schema,
            target=config["target"],
            features=config.get("features"),
            exclude_features=config.get("exclude_features")
        )
        
        # 4. Train model
        pipeline, metrics = train_model_with_validation(
            df=df,
            features=features,
            target=config["target"],
            task_type=config["task_type"],
            xgb_params=config.get("xgb_params", {})
        )
        
        # 5. Generate model ID
        model_id = generate_id()
        
        # 6. Save model artifact
        artifact_path = ARTIFACTS_DIR / f"model_{model_id}.joblib"
        joblib.dump(pipeline, artifact_path)
        
        # 7. Save model metadata
        metadata = {
            "model_id": model_id,
            "user_id": user_id,
            "username": config.get("username", ""),
            "model_name": config["model_name"],
            "task_type": config["task_type"],
            "target": config["target"],
            "features": features,
            "xgb_params": config.get("xgb_params", {}),
            "dataset_id": config["dataset_id"],
            "created_at": datetime.utcnow().isoformat() + "Z",
            "training_duration": time.time() - start_time,
            "row_count": len(df),
            "feature_count": len(features),
            "metrics": metrics,
            "evaluation_method": "train_test_split"
        }
        
        metadata_path = MODEL_METADATA_DIR / f"model_{model_id}.json"
        atomic_write_json(metadata_path, metadata)
        
        # 8. Save model schema
        model_schema = {
            "model_id": model_id,
            "user_id": user_id,
            "dataset_name": config.get("dataset_name", ""),
            "numeric_features": schema["numeric_features"],
            "categorical_features": schema["categorical_features"]
        }
        
        schema_path = MODEL_SCHEMAS_DIR / f"model_{model_id}_schema.json"
        atomic_write_json(schema_path, model_schema)
        
        # 9. Update job status to succeeded
        training_duration = time.time() - start_time
        update_job_status(
            job_id,
            "succeeded",
            result={
                "model_id": model_id,
                "metrics": metrics,
                "training_duration": training_duration
            }
        )
        
        return model_id
        
    except Exception as e:
        # Update job status to failed
        error_info = {
            "error_type": type(e).__name__,
            "message": str(e),
            "details": None
        }
        
        update_job_status(
            job_id,
            "failed",
            error=error_info
        )
        
        # Re-raise for Celery retry mechanism
        raise
```

> **é‡é»**:
> - ä½¿ç”¨ `@celery_app.task(bind=True, max_retries=3)` æ”¯æ´é‡è©¦
> - æ›´æ–° job ç‹€æ…‹ï¼šqueued â†’ running â†’ succeeded/failed
> - è¤‡ç”¨ç¾æœ‰çš„è¨“ç·´é‚è¼¯ (`train_model_with_validation`)
> - éŒ¯èª¤è™•ç†ï¼šæ•ç²ç•°å¸¸ä¸¦è¨˜éŒ„åˆ° job

---

#### [NEW] [app/tasks/__init__.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/tasks/__init__.py)
```python
"""Celery tasks module"""
from app.tasks.training_tasks import train_model_task

__all__ = ["train_model_task"]
```

---

### Phase 4: API ç«¯é»ä¿®æ”¹

#### [MODIFY] [app/routers/training.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/routers/training.py)
å°‡ `POST /train` æ”¹ç‚ºéåŒæ­¥ï¼š

**ä¿®æ”¹ç¯„åœ**: æ•´å€‹ `train_model` å‡½æ•¸ (line 33-224)

**ä¸»è¦æ”¹è®Š**:
1. å›å‚³é¡å‹ï¼š`TrainResponse` â†’ `JobCreateResponse`
2. ä¸å†åŸ·è¡Œè¨“ç·´ï¼Œæ”¹ç‚ºå‰µå»º job ä¸¦é€å…¥ Celery queue
3. ç«‹å³å›å‚³ `job_id`

```python
from app.models.schemas import JobCreateResponse
from app.services.job_service import create_job
from app.tasks.training_tasks import train_model_task

@router.post("", response_model=JobCreateResponse)
@limiter.limit("3/minute")
async def train_model(
    request: Request,
    train_request: TrainRequest,
    current_user: dict = Depends(verify_api_key)
):
    """
    Submit a model training job (asynchronous).
    
    **Breaking Change (v2.0)**: This endpoint now returns a job_id instead of model_id.
    Use GET /jobs/{job_id} to check training status and retrieve the model_id when complete.
    
    - Creates a training job
    - Queues the job to Celery worker
    - Returns immediately with job_id
    """
    # Validate dataset exists
    dataset_path = DATASETS_DIR / f"{train_request.dataset_id}.csv"
    if not dataset_path.exists():
        raise HTTPException(
            status_code=404,
            detail={
                "error": "dataset_not_found",
                "message": f"Dataset {train_request.dataset_id} not found",
                "details": None
            }
        )
    
    # Load dataset schema
    schema_path = DATASET_METADATA_DIR / f"schema_{train_request.dataset_id}.json"
    if not schema_path.exists():
        raise HTTPException(
            status_code=404,
            detail={
                "error": "schema_not_found",
                "message": f"Schema for dataset {train_request.dataset_id} not found",
                "details": None
            }
        )
    
    # Prepare training configuration
    train_config = {
        "dataset_id": train_request.dataset_id,
        "model_name": train_request.model_name,
        "task_type": train_request.task_type,
        "target": train_request.target,
        "features": train_request.features,
        "exclude_features": train_request.exclude_features,
        "xgb_params": train_request.xgb_params or {},
        "username": current_user.get("username", "")
    }
    
    # Queue training task to Celery
    task = train_model_task.delay(job_id=None)  # Will be updated after job creation
    
    # Create job record
    job_data = create_job(
        user_id=train_request.user_id,
        train_config=train_config,
        celery_task_id=task.id
    )
    
    # Update task with job_id (hack: store in task args)
    task.update_state(meta={'job_id': job_data['job_id']})
    
    # Actually queue the task with job_id
    task = train_model_task.apply_async(args=[job_data['job_id']])
    
    # Update job with correct task_id
    from app.services.job_service import update_job_status
    update_job_status(job_data['job_id'], 'queued', celery_task_id=task.id)
    
    return JobCreateResponse(
        job_id=job_data['job_id'],
        status="queued",
        created_at=job_data['created_at']
    )
```

> **æ³¨æ„**: ç§»é™¤äº†åŸæœ¬çš„è¨“ç·´é‚è¼¯ï¼Œæ”¹ç‚ºå‰µå»º job ä¸¦é€å…¥ queueã€‚

---

#### [NEW] [app/routers/jobs.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/routers/jobs.py)
æ–°å¢ Jobs ç®¡ç†ç«¯é»ï¼š

```python
"""Job management endpoints"""
from fastapi import APIRouter, HTTPException, Depends, Request, Query
from typing import Optional

from app.models.schemas import (
    JobDetail, JobListResponse, JobSummary,
    ErrorResponse
)
from app.services.job_service import (
    get_job, list_jobs, delete_job, update_job_status
)
from app.utils.auth import verify_api_key
from app.utils.rate_limit import limiter
from app.celery_app import celery_app

router = APIRouter(prefix="/jobs", tags=["jobs"])


@router.get("", response_model=JobListResponse)
@limiter.limit("120/minute")
async def list_training_jobs(
    request: Request,
    user_id: Optional[str] = None,
    status: Optional[str] = None,
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """
    List training jobs with optional filters.
    
    **Rate Limit**: 120 per minute (IP-based)
    
    **Query Parameters:**
    - **user_id** (optional): Filter by user ID
    - **status** (optional): Filter by status (queued/running/succeeded/failed)
    - **limit** (default: 50, max: 100): Number of results
    - **offset** (default: 0): Pagination offset
    """
    jobs_data, total = list_jobs(
        user_id=user_id,
        status=status,
        limit=limit,
        offset=offset
    )
    
    # Convert to JobSummary
    job_summaries = []
    for job in jobs_data:
        summary = JobSummary(
            job_id=job['job_id'],
            job_type=job.get('job_type', 'train'),
            status=job['status'],
            user_id=job['user_id'],
            model_name=job['train_config'].get('model_name', ''),
            created_at=job['created_at'],
            completed_at=job.get('completed_at'),
            model_id=job.get('result', {}).get('model_id') if job.get('result') else None
        )
        job_summaries.append(summary)
    
    return JobListResponse(
        jobs=job_summaries,
        total=total,
        limit=limit,
        offset=offset
    )


@router.get("/{job_id}", response_model=JobDetail)
@limiter.limit("120/minute")
async def get_job_status(request: Request, job_id: str):
    """
    Get detailed status of a training job.
    
    **Rate Limit**: 120 per minute (IP-based)
    
    Returns complete job information including:
    - Current status (queued/running/succeeded/failed)
    - Training configuration
    - Result (model_id, metrics) when succeeded
    - Error details when failed
    """
    job_data = get_job(job_id)
    
    if not job_data:
        raise HTTPException(
            status_code=404,
            detail={
                "error": "job_not_found",
                "message": f"Job {job_id} not found",
                "details": None
            }
        )
    
    return JobDetail(**job_data)


@router.delete("/{job_id}")
@limiter.limit("30/minute")
async def cancel_job(
    request: Request,
    job_id: str,
    current_user: dict = Depends(verify_api_key)
):
    """
    Cancel or delete a training job.
    
    **Requires Authentication**: X-API-Key header
    **Rate Limit**: 30 per minute (API Key-based)
    
    Behavior:
    - **queued**: Remove from queue and mark as cancelled
    - **running**: Attempt to cancel (may not stop immediately)
    - **succeeded/failed**: Delete job record (model is preserved)
    """
    job_data = get_job(job_id)
    
    if not job_data:
        raise HTTPException(
            status_code=404,
            detail={
                "error": "job_not_found",
                "message": f"Job {job_id} not found",
                "details": None
            }
        )
    
    # Verify ownership
    if job_data['user_id'] != current_user['user_id']:
        raise HTTPException(
            status_code=403,
            detail={
                "error": "forbidden",
                "message": "You don't have permission to cancel this job",
                "details": None
            }
        )
    
    previous_status = job_data['status']
    
    # Cancel Celery task if queued or running
    if previous_status in ['queued', 'running']:
        celery_task_id = job_data.get('celery_task_id')
        if celery_task_id:
            celery_app.control.revoke(celery_task_id, terminate=True)
        
        update_job_status(job_id, 'cancelled')
        message = "Job cancelled successfully"
    else:
        # Delete completed/failed jobs
        delete_job(job_id)
        message = "Job deleted successfully"
    
    return {
        "message": message,
        "job_id": job_id,
        "previous_status": previous_status
    }
```

---

#### [MODIFY] [app/main.py](file:///c:/Users/ander/Documents/GitHub/XGenerator/app/main.py)
å¼•å…¥ jobs routerï¼š

```python
from app.routers import datasets, training, prediction, models, users, jobs  # NEW

# Include routers
app.include_router(users.router)
app.include_router(datasets.router)
app.include_router(training.router)
app.include_router(prediction.router)
app.include_router(models.router)
app.include_router(jobs.router)  # NEW
```

ä¸¦æ›´æ–°ç‰ˆæœ¬è™Ÿï¼š

```python
app = FastAPI(
    title="XGBoost Training Service API",
    version="2.0.0",  # Breaking change: async training
    # ...
)
```

---

#### [NEW] [Dockerfile](file:///c:/Users/ander/Documents/GitHub/XGenerator/Dockerfile)
å‰µå»º Dockerfileï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼š

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Default command (overridden in docker-compose)
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

### Phase 5: æ¸¬è©¦èˆ‡é©—è­‰

## é©—è­‰è¨ˆç•«

### 1. è‡ªå‹•åŒ–æ¸¬è©¦

#### å–®å…ƒæ¸¬è©¦: Job Service
**æ–°å¢æ¸¬è©¦**: `tests/unit/test_job_service.py`

```python
"""Unit tests for job service"""
import pytest
import json
from pathlib import Path
from app.services.job_service import (
    create_job, get_job, update_job_status, list_jobs, delete_job
)
from app.config import JOBS_DIR

def test_create_job():
    """Test job creation"""
    train_config = {
        "dataset_id": "test123",
        "model_name": "Test Model",
        "task_type": "classification",
        "target": "label"
    }
    
    job_data = create_job(
        user_id="user_123",
        train_config=train_config,
        celery_task_id="task_abc"
    )
    
    assert job_data['status'] == 'queued'
    assert job_data['user_id'] == 'user_123'
    assert job_data['train_config'] == train_config
    
    # Verify file exists
    job_path = JOBS_DIR / f"job_{job_data['job_id']}.json"
    assert job_path.exists()

def test_update_job_status():
    """Test job status update"""
    # Create job first
    job_data = create_job(
        user_id="user_123",
        train_config={},
        celery_task_id="task_abc"
    )
    
    # Update to running
    update_job_status(job_data['job_id'], 'running')
    updated = get_job(job_data['job_id'])
    assert updated['status'] == 'running'
    assert updated['started_at'] is not None
    
    # Update to succeeded
    update_job_status(
        job_data['job_id'],
        'succeeded',
        result={'model_id': 'model_xyz'}
    )
    final = get_job(job_data['job_id'])
    assert final['status'] == 'succeeded'
    assert final['completed_at'] is not None
    assert final['result']['model_id'] == 'model_xyz'
```

**åŸ·è¡Œå‘½ä»¤**:
```bash
pytest tests/unit/test_job_service.py -v
```

---

#### æ•´åˆæ¸¬è©¦: éåŒæ­¥è¨“ç·´æµç¨‹
**ä¿®æ”¹**: `tests/integration/test_end_to_end.py`

æ–°å¢æ¸¬è©¦å‡½æ•¸ `test_async_training_workflow`:

```python
def test_async_training_workflow():
    """Test complete async training workflow"""
    print_step(7, "Test Async Training Workflow (v2.0)")
    
    # 1. Create user and upload dataset (reuse existing)
    # ...
    
    # 2. Submit training job
    train_payload = {
        "user_id": user_id,
        "model_name": "Async Test Model",
        "dataset_id": dataset_id,
        "task_type": "classification",
        "target": "Survived",
        "features": None,
        "xgb_params": {"n_estimators": 50, "max_depth": 3}
    }
    
    response = requests.post(
        f"{BASE_URL}/train",
        json=train_payload,
        headers={"X-API-Key": api_key}
    )
    
    assert response.status_code == 200, "Training job creation failed"
    job_data = response.json()
    assert "job_id" in job_data
    assert job_data["status"] == "queued"
    
    job_id = job_data["job_id"]
    print_success(f"Job created: {job_id}")
    
    # 3. Poll job status until complete
    max_wait = 120  # 2 minutes
    poll_interval = 5
    elapsed = 0
    
    while elapsed < max_wait:
        response = requests.get(f"{BASE_URL}/jobs/{job_id}")
        assert response.status_code == 200
        
        job_status = response.json()
        print_info(f"Job status: {job_status['status']}")
        
        if job_status['status'] == 'succeeded':
            assert 'result' in job_status
            assert 'model_id' in job_status['result']
            model_id = job_status['result']['model_id']
            print_success(f"Training succeeded! Model ID: {model_id}")
            break
        elif job_status['status'] == 'failed':
            pytest.fail(f"Training failed: {job_status.get('error')}")
        
        time.sleep(poll_interval)
        elapsed += poll_interval
    else:
        pytest.fail("Training timeout")
    
    # 4. Verify model exists
    response = requests.get(f"{BASE_URL}/models/{model_id}")
    assert response.status_code == 200
    print_success("Model verified successfully")
```

**åŸ·è¡Œå‘½ä»¤**:
```bash
# éœ€è¦å…ˆå•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose up -d

# ç­‰å¾…æœå‹™å°±ç·’
sleep 10

# åŸ·è¡Œæ•´åˆæ¸¬è©¦
pytest tests/integration/test_end_to_end.py::test_async_training_workflow -v
```

---

### 2. æ‰‹å‹•æ¸¬è©¦

#### æ¸¬è©¦æ­¥é©Ÿ

**å‰ç½®æ¢ä»¶**: å•Ÿå‹•æ‰€æœ‰æœå‹™
```bash
docker-compose up --build
```

**æ¸¬è©¦æµç¨‹**:

1. **å‰µå»ºç”¨æˆ¶**
   ```bash
   curl -X POST http://localhost:8000/users \
     -H "Content-Type: application/json" \
     -d '{"username": "testuser", "email": "test@example.com"}'
   ```
   è¨˜ä¸‹ `api_key`

2. **ä¸Šå‚³è³‡æ–™é›†**
   ```bash
   curl -X POST http://localhost:8000/datasets \
     -H "X-API-Key: YOUR_API_KEY" \
     -F "file=@train_full.csv" \
     -F "user_id=YOUR_USER_ID" \
     -F "dataset_name=Titanic Dataset"
   ```
   è¨˜ä¸‹ `dataset_id`

3. **æäº¤è¨“ç·´ä»»å‹™**
   ```bash
   curl -X POST http://localhost:8000/train \
     -H "X-API-Key: YOUR_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{
       "user_id": "YOUR_USER_ID",
       "model_name": "Titanic Classifier",
       "dataset_id": "YOUR_DATASET_ID",
       "task_type": "classification",
       "target": "Survived",
       "features": null,
       "xgb_params": {"n_estimators": 100, "max_depth": 5}
     }'
   ```
   **é æœŸçµæœ**: ç«‹å³å›å‚³ job_idï¼Œç‹€æ…‹ç‚º `queued`

4. **æŸ¥è©¢ä»»å‹™ç‹€æ…‹**
   ```bash
   curl http://localhost:8000/jobs/YOUR_JOB_ID
   ```
   **é æœŸç‹€æ…‹è½‰æ›**: queued â†’ running â†’ succeeded

5. **åˆ—å‡ºæ‰€æœ‰ä»»å‹™**
   ```bash
   curl http://localhost:8000/jobs
   ```

6. **é©—è­‰æ¨¡å‹**
   ```bash
   curl http://localhost:8000/models/YOUR_MODEL_ID
   ```

7. **å–æ¶ˆä»»å‹™ï¼ˆå¯é¸ï¼‰**
   ```bash
   curl -X DELETE http://localhost:8000/jobs/YOUR_JOB_ID \
     -H "X-API-Key: YOUR_API_KEY"
   ```

---

### 3. ç›£æ§é©—è­‰

#### æª¢æŸ¥ Worker Logs
```bash
docker-compose logs -f worker
```

**é æœŸè¼¸å‡º**:
```
worker_1  | [2024-01-01 00:00:00,000: INFO/MainProcess] Connected to redis://redis:6379/0
worker_1  | [2024-01-01 00:00:05,000: INFO/MainProcess] Task train_model_task[abc123] received
worker_1  | [2024-01-01 00:00:45,000: INFO/MainProcess] Task train_model_task[abc123] succeeded
```

#### æª¢æŸ¥ Redis
```bash
docker-compose exec redis redis-cli
> KEYS *
> GET celery-task-meta-XXX
```

---

### 4. ä¸¦ç™¼è¨“ç·´æ¸¬è©¦

**æ¸¬è©¦ç›®æ¨™**: é©—è­‰å¯ä»¥åŒæ™‚è¨“ç·´å¤šå€‹æ¨¡å‹

**æ­¥é©Ÿ**:
1. å¿«é€Ÿé€£çºŒæäº¤ 3 å€‹è¨“ç·´ä»»å‹™
2. è§€å¯Ÿ worker logsï¼Œç¢ºèªæœ‰ 2 å€‹ä»»å‹™åŒæ™‚åŸ·è¡Œï¼Œ1 å€‹æ’éšŠ
3. ç­‰å¾…å…¨éƒ¨å®Œæˆï¼Œé©—è­‰ 3 å€‹æ¨¡å‹éƒ½æˆåŠŸè¨“ç·´

**é©—è­‰å‘½ä»¤**:
```bash
# æäº¤ç¬¬1å€‹ä»»å‹™
curl -X POST http://localhost:8000/train ... > job1.json

# æäº¤ç¬¬2å€‹ä»»å‹™
curl -X POST http://localhost:8000/train ... > job2.json

# æäº¤ç¬¬3å€‹ä»»å‹™
curl -X POST http://localhost:8000/train ... > job3.json

# åŒæ™‚æŸ¥è©¢
curl http://localhost:8000/jobs | jq '.jobs[] | {job_id, status}'
```

---

### 5. å¤±æ•—å ´æ™¯æ¸¬è©¦

#### æ¸¬è©¦ 1: Invalid Dataset
æäº¤è¨“ç·´ä»»å‹™æ™‚ä½¿ç”¨ä¸å­˜åœ¨çš„ dataset_id

**é æœŸ**: ç«‹å³å›å‚³ 404 errorï¼ˆä¸å‰µå»º jobï¼‰

#### æ¸¬è©¦ 2: Invalid Target Column
ä½¿ç”¨ä¸å­˜åœ¨çš„ target column

**é æœŸ**: Job ç‹€æ…‹è®Šç‚º `failed`ï¼Œerror è¨˜éŒ„éŒ¯èª¤è¨Šæ¯

#### æ¸¬è©¦ 3: Worker Crash
æ‰‹å‹•åœæ­¢ workerï¼Œç„¶å¾Œé‡å•Ÿ

**é æœŸ**: æ­£åœ¨åŸ·è¡Œçš„ä»»å‹™æœƒé‡è©¦æˆ–å¤±æ•—ï¼Œqueued ä»»å‹™æœƒè¢«æ–° worker æ¥æ‰‹

---

## ğŸ“‹ å¯¦ä½œæª¢æŸ¥æ¸…å–®

å®Œæˆå¾Œç¢ºèªï¼š

- [ ] Docker Compose å¯æ­£å¸¸å•Ÿå‹•ï¼ˆ3å€‹æœå‹™éƒ½ runningï¼‰
- [ ] Redis é€£ç·šæ­£å¸¸
- [ ] Worker å¯å¾ queue å–å‡ºä»»å‹™
- [ ] POST /train ç«‹å³å›å‚³ job_id
- [ ] GET /jobs/{job_id} å¯æŸ¥è©¢ç‹€æ…‹
- [ ] è¨“ç·´å®Œæˆå¾Œ job ç‹€æ…‹è®Šç‚º succeeded
- [ ] æ¨¡å‹æª”æ¡ˆå’Œ metadata æ­£ç¢ºå„²å­˜
- [ ] å¯åŒæ™‚è¨“ç·´å¤šå€‹æ¨¡å‹ï¼ˆä¸¦ç™¼ï¼‰
- [ ] éŒ¯èª¤è™•ç†æ­£ç¢ºï¼ˆfailed ç‹€æ…‹ + error è¨Šæ¯ï¼‰
- [ ] å–®å…ƒæ¸¬è©¦é€šé
- [ ] æ•´åˆæ¸¬è©¦é€šé

---

## ğŸš§ å·²çŸ¥é™åˆ¶èˆ‡æœªä¾†æ”¹é€²

### ç›®å‰ä¸å¯¦ä½œçš„åŠŸèƒ½
1. **è‡ªå‹•æ¸…ç†èˆŠ Job** - ç­‰éœ€è¦æ™‚å†åŠ ï¼ˆå®šæœŸåˆªé™¤ 90å¤©å‰çš„è¨˜éŒ„ï¼‰
2. **Celery Flower** - ç›£æ§ä»‹é¢ï¼Œéœ€è¦æ™‚ 5 åˆ†é˜å¯åŠ å…¥
3. **é€²åº¦æ›´æ–°** - ä»»å‹™åŸ·è¡Œç™¾åˆ†æ¯”ï¼ˆéœ€è¦ä¿®æ”¹è¨“ç·´é‚è¼¯ï¼‰
4. **WebSocket é€šçŸ¥** - å³æ™‚æ¨é€ç‹€æ…‹è®Šæ›´ï¼ˆç›®å‰é è¼ªè©¢ï¼‰

### æ³¨æ„äº‹é …
- é€™æ˜¯ **Breaking Change**ï¼Œéœ€è¦æ›´æ–°æ‰€æœ‰å®¢æˆ¶ç«¯ä»£ç¢¼
- Worker å’Œ API å…±äº« `./data` ç›®éŒ„ï¼ˆDocker volumeï¼‰
- Redis ä¸æŒä¹…åŒ–ï¼ˆé‡å•Ÿå¾Œ queue æ¸…ç©ºï¼‰
